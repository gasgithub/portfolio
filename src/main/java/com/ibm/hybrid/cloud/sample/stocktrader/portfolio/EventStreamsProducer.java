/*
       Copyright 2017 IBM Corp All Rights Reserved

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
 */

package com.ibm.hybrid.cloud.sample.stocktrader.portfolio;

import java.net.ConnectException;
import java.util.Properties;
import java.util.concurrent.ExecutionException;
import java.util.logging.Logger;

import javax.annotation.PostConstruct;
import javax.annotation.PreDestroy;
import javax.enterprise.context.ApplicationScoped;
import javax.inject.Inject;
import javax.json.bind.Jsonb;
import javax.json.bind.JsonbBuilder;

import org.apache.kafka.clients.CommonClientConfigs;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.KafkaException;
import org.apache.kafka.common.config.SaslConfigs;
import org.apache.kafka.common.config.SslConfigs;
import org.apache.kafka.common.serialization.StringSerializer;
import org.eclipse.microprofile.config.inject.ConfigProperty;


/** Producer class generated by the Event Streams sample producer generator.  I've left as-is, except for
 *  adding the Apache license, renaming the class, switching from log4j to java.util.logging, and renaming
 *  the two environment variables to conform to my naming conventions.
 */
@ApplicationScoped
public class EventStreamsProducer {
	
	private @Inject @ConfigProperty(name = "KAFKA_TOPIC", defaultValue = "stock-purchased") String kafkaTopic;
	private @Inject @ConfigProperty(name = "KAFKA_ADDRESS", defaultValue = "") String kafkaAddress;
	private @Inject @ConfigProperty(name = "KAFKA_API_KEY", defaultValue = "") String API_KEY;

    private @Inject @ConfigProperty(name = "KAFKA_USER", defaultValue = "token") String USERNAME;
    private String KEYSTORE = System.getenv("KAFKA_KEYSTORE");

    private KafkaProducer<String, String> kafkaProducer;
    
    private Jsonb jsonb = JsonbBuilder.create();
    
    private Logger logger = Logger.getLogger(EventStreamsProducer.class.getName());
    
    @PostConstruct
    void initialize() {
    	System.out.println("initializing...");
    	System.out.println(kafkaAddress);
    	System.out.println(kafkaTopic);
    	System.out.println(API_KEY);
    	
        Properties properties = new Properties();
        properties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, kafkaAddress);
        properties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, "SASL_SSL");
        properties.put(CommonClientConfigs.CONNECTIONS_MAX_IDLE_MS_CONFIG, 10000);
        properties.put(ProducerConfig.MAX_BLOCK_MS_CONFIG, 4000);
        properties.put(ProducerConfig.RETRIES_CONFIG, 0);
        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class); //JSONSerializer.class);
//      properties.put(SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, "");
        properties.put(SslConfigs.SSL_PROTOCOL_CONFIG, "TLSv1.2");
//        properties.put(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, KEYSTORE);
//        properties.put(SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, "password");
        properties.put(SaslConfigs.SASL_MECHANISM, "PLAIN");
        String saslJaasConfig = "org.apache.kafka.common.security.plain.PlainLoginModule required username=\""
            + USERNAME + "\" password=\"" + API_KEY + "\";";
        properties.put(SaslConfigs.SASL_JAAS_CONFIG, saslJaasConfig);
        System.out.println(saslJaasConfig);
        try {
        	System.out.println("initializing KafkaProducer");
        	kafkaProducer = new KafkaProducer<>(properties);
        } catch (Exception kafkaError ) {
            logger.warning("Error while creating producer: "+kafkaError.getMessage());
            Throwable cause = kafkaError.getCause();
            if (cause != null) {
            	logger.warning("Caused by: "+cause.getMessage());
            }
            throw kafkaError;
        }
        System.out.println("kafkaProducer created");
    	
    }


    public RecordMetadata produce(Object event) throws InterruptedException, ExecutionException, ConnectException {
        String eventAsString = jsonb.toJson(event);	
        ProducerRecord<String, String> record = new ProducerRecord<>(kafkaTopic, null, eventAsString);
        RecordMetadata recordMetadata = kafkaProducer.send(record).get();
        logger.info("Message produced: " +recordMetadata);
        return recordMetadata;
    }

    /* Poducer to specific topic */
    public RecordMetadata produce(Object event, String topic) throws InterruptedException, ExecutionException, ConnectException {
        String eventAsString = jsonb.toJson(event);	
        ProducerRecord<String, String> record = new ProducerRecord<>(topic, null, eventAsString);
        RecordMetadata recordMetadata = kafkaProducer.send(record).get();
        logger.info("Message produced: " +recordMetadata);
        return recordMetadata;
    }
    
    @PreDestroy
    public void shutdown() {
    	logger.info("Destroying producer");
        kafkaProducer.flush();
        kafkaProducer.close();
    }
    
}
